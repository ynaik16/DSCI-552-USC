# -*- coding: utf-8 -*-
"""DSCI552.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F5DptX6CkPM53h7owun3kqJLW5WOVhF5
"""

import json
from math  import log
import pandas as pd
import math

# import and pre-processing the dataset
def import_dataset(filename):
    f = open(filename)
    
    dataset = []
    features = []
    
    # remove the unnecessary characters '(), ' from data
    labels = f.readline()
    labels = labels.strip('()').split(',')
    for word in labels :
        word = word.strip()
        features.append(word)      
    features = features[:-1]
    
    rows = f.readlines()[1:] 
    for row in rows:      
        row = row[4:].strip().strip(';').split(',')
        row_list = []
        for word in row: 
            row_list.append(word.strip())
        dataset.append(row_list)
        
    return (dataset, features)

# calculate the entropy of each dataset

def calculate_entropy(dataset):
    df = pd.DataFrame(dataset)
    target = df.iloc[: , -1]
    attributes = target.unique()
    entr = 0
    
    for attr in attributes:
        p = target.value_counts()[attr] / len(target)
        entr += -p*math.log2(p)

    return entr

# split the dataset into children set, so we could calculate the entropy of each feature
def split_dataset(dataset, index, feature_value):    
    child_set = []
    
    for row in dataset:
        if row[index] == feature_value:
            new_row = row[:index] + row[index+1:]
            child_set.append(new_row)
            
    return child_set

# find the best feature to split
def select_best_feature(dataset):
    
    num_features = len(dataset[0]) - 1
    
    base_entropy = calculate_entropy(dataset)
    
    # initialize the max information gain as 0
    max_IG = 0      
    # initialize the location of best split feature
    location = -1
    
    for i in range(num_features):
        feature_value = [row[i] for row in dataset]
        unique_feature_value = set(feature_value)
        
        children_entropy = 0 
        
        for value in unique_feature_value:
            child_set = split_dataset(dataset, i, value)
            child_set_entropy = calculate_entropy(child_set)
            p = len(child_set) / len(dataset)
            children_entropy += p * child_set_entropy
            
        # calculate information gain of this feature 
        feature_info_gain = base_entropy - children_entropy 
        
        # update once we find the feature with more information gain
        if feature_info_gain > max_IG:
            max_IG = feature_info_gain
            location = i
    
    return location

# create a new decision tree
def build_decision_tree(dataset, features):
    
    label_value = [row[-1] for row in dataset]
    if len(set(label_value)) == 1:    
        return label_value[0]  
    
    location = select_best_feature(dataset)
    features_list = features[:]
    splited_feature = features[location]
    
    decision_tree = {}
    decision_tree[splited_feature] = {}
    features_list.remove(splited_feature)
    
    values = set([row[location] for row in dataset])

    for value in values:
        child_set = split_dataset(dataset, location, value)
        child_set_features = features_list[:]
        # build decision tree by recursion
        decision_tree[splited_feature][value] = build_decision_tree(child_set, child_set_features)

    return decision_tree

def predict(tree, feature_labels, test_data):
    
    first_feature = list(tree)[0]
    first_feature_value_tree = tree[first_feature]
    feature_index = feature_labels.index(first_feature)
    test_value = test_data[feature_index]
    value_tree = first_feature_value_tree[test_value]
    
    if isinstance(value_tree, dict): 
        label = predict(value_tree, feature_labels, test_data)
    else: 
        label = value_tree
        
    return label

if __name__=='__main__':
    
    datasets = import_dataset('data.txt')

    dataset = datasets[0]
    features = datasets[1]
    
    tree = build_decision_tree(dataset, features)
    print('Tree: \n{}'.format(json.dumps(tree, indent = 6)))
    
    # Make prediction
    test_data = ["Moderate", "Cheap", "Loud", "City-Center", "No", "No"]
    result = predict(tree, features, test_data)
    print('\n The result of prediction is {}'.format(result))













